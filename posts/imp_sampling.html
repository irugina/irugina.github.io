---
layout: post
---

<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
</style>

<h1>Not All Samples Are Created Equal: Deep Learning with Importance Sampling [1]   - June 21, 2020</h1>

Introduce an importance sampling scheme that 


<ul>
  <li>focuses computation on important exemples  </li>
  <li>reduces the variance of stochastic gradients during training </li>
</ul>

Main contributions:


<ul>
  <li>tractable upper bound on per-sample gradient norm  </li>
  <li>estimator of achieved variance reduction - decide when to employ sampling scheme </li>
</ul>

<h2>Related work</h2>


<h3> Variance Reduction [2]</h3>

Computing a Monte-Carlo estimate:

\begin{equation}
\label{eq:estimator}
    \mathbb{E}_{p(x)} \left[f(x) \right] = \int p(x) f(x) dx \approx \frac{1}{N} \sum_{i=1}^N f(x_i)
\end{equation}

Consider 

\begin{equation*}
    \mathbb{E}_{q(x)} \left[ \frac{p(x)}{q(x)}f(x) \right]= \int q(x) \frac{p(x)}{q(x)} f(x) dx = \mathbb{E}_{p(x)} \left [f(x) \right]
\end{equation*}

so 

\begin{equation*}
    \frac{1}{N} \sum \frac{p(x_n)} {q(x_n)} f(x_n) \text{, where $x_n \sim q$, is an unbiased estimator for Eq. \ref{eq:estimator}.}
\end{equation*}
<em>
Variance is minimized when $q^*(x) \propto p(x) |f(x)|$. [2] generalize this for multi-dimensional case where minimizing the variance $\rightarrow$ minimizing $\text{Tr} (\Sigma)$, the trace of co-variance matrix. 
</em> <br>

This sets a theoretical framework for this problem - importance sampling in this form is however intractable because of all the gradient norm calculations: for us $f(x) = \nabla \mathcal{L}$.

<h3>Deep Learning</h3>
It is common for deep embedding learning to sample hard examples because of the plethora of easy non informative ones. Other work keeps a history of observed losses and samples based on that. Main drawback here is that this approach requires optimizing numerous hyper-parameters. 

<h2>Why variance reduction - convergence speed</h2>

Let $P_t$ be the sampling distribution at time $t$. Define  <em>convergence speed</em> $S$ of SGD as 
\begin{equation}
\label{eq:conv_speed}
    S = -\mathbb{E}_{P_t} \left[ ||\theta_{t+1} - \theta^*||_2^2 - ||\theta_{t} - \theta^*||_2^2 \right]
\end{equation}

where the update at time $t$ was made after sampling datapoint $I_t$ from $P_t$ and has the form:

\begin{equation*}
    \theta_{t+1} = \theta_{t} - \eta w_{I_t} \nabla{\theta_t} \mathcal{L}\left(\Phi(x_{I_t}, \theta_{t}), y_{I_t}\right)
\end{equation*}

Note the re-scaling coefficient $w_{I_t} = \frac{1}{N p_i^t}$ (want unbiased estimator for batch gradient). In uniform SGD $P_t$ is uniform, $p_i^t = \frac{1}{N}$, and $w_{I_t} = 1$.

<br><br>

Let $G_i =w_{i} \nabla{\theta_t} \mathcal{L}\left(\Phi(x_i,\theta_{t}), y_i \right) $. After some algebra Eq. \ref{eq:conv_speed} becomes:

\begin{equation*}
    S = 2 \eta (\theta_t - \theta^*) \mathbb{E}_{P_t} [G_{I_t}] - \eta^2 \mathbb{E}_{P_t} [G_{I_t}]^T\mathbb{E}_{P_t} [G_{I_t}] - \eta^2 \text{Tr}(\mathbb{V}_{P_t}[G_{I_t}])
\end{equation*}

First two terms are the speed
of batch gradient descent. It is possible to gain a speedup by sampling from the distribution $P_t$ that
minimizes $\text{Tr}(\mathbb{V}_{P_t}[G_{I_t}])$, which[2] tells us is $P_t \propto |\nabla{\theta_t} \mathcal{L}\left(\Phi(x_i,\theta_{t}), y_i \right)|$ 

<h2>Beyond the Full Gradient Norm</h2>

<h3>Chain Rule</h3>

Find an upper bound on gradient norms $|\nabla_{\theta_t} \mathcal{L}\left(\Phi(x_i,\theta_{t}), y_i \right)| \leq \hat{G_i}$. 

Let $\mathcal{L} \equiv  \mathcal{L}\left(\Phi(x_i,\theta_{t}), y_i \right) $ and look at gradient w.r.t. parameters $\theta^{(l)} $ at layer $l$:

\begin{equation}
\label{eq:chain_rule}
    \nabla_{\theta^{(l)} } \mathcal{L}=  \frac{\partial x^{(l)}} {\partial z^{(l)}} \cdot  \left( \frac{\partial z^{(l+1)}} {\partial x^{(l)}} \right)^T \cdot  \frac{\partial x^{(l+1)}} {\partial z^{(l+1)}} \;... \; \\ \left( \frac{\partial z^{(L)}} {\partial x^{(L-1)}} \right)^T \cdot \frac{\partial x^{(L)}} {\partial z^{(L)}} \cdot \nabla_{x^{(L)} } \mathcal{L} \cdot \left (\frac{\partial z^{(l)}} {\partial \theta^{(l)}} \right) ^T
\end{equation}




Activation functions:

\begin{equation*}
   x^{(l)} = \sigma^{(l)}(z^{(l)}) \implies \frac{\partial  x^{(l)}}{\partial z^{(l)}} = \text{diag} \left(\sigma^{'(l)}(z^{(l)})\right) \equiv \Sigma'_l
\end{equation*}

Fully connected layers:

\begin{equation*}
   z^{(l)} = \theta^{(l)}  x^{(l-1)} \implies \frac{\partial  z^{(l)}}{\partial  x^{(l-1)} } = \theta^{(l)} 
\end{equation*}

Last term in Eq. \ref{eq:chain_rule} :

\begin{equation*}
   z^{(l)} = \theta^{(l)}  x^{(l-1)} \implies \frac{\partial  z^{(l)}}{\partial  \theta^{(l)} } = x^{(l-1)}
\end{equation*}

Going back to Eq. \ref{eq:chain_rule} the gradient  becomes:

\begin{equation*}
    \nabla_{\theta^{(l)} } \mathcal{L} =  \Sigma'_l \cdot  \theta_{(l+1)}^T  \cdot \Sigma'_{l+1} \cdot \; ... \; \theta_{L}^T \cdot \Sigma'_L \nabla_{x^{(L)} } \mathcal{L} \cdot x_{(l-1)}^T
\end{equation*}

<h3>Upper bound</h3>

Let $\Delta^{(l)} = \Sigma'_l \cdot  \theta_{(l+1)}^T  \cdot \Sigma'_{l+1} \cdot \; ... \; \theta_{L}^T $. We have $\nabla_{\theta^{(l)} } \mathcal{L} = \Delta^{(l)} \Sigma'_L \nabla_{x^{(L)} } \mathcal{L} \cdot x_{(l-1)}^T$ and 

\begin{equation*}
    ||\nabla_{\theta^{(l)} } \mathcal{L}||_2 \leq ||\Delta^{(l)}||_2  \cdot ||x_{(l-1)}||_2 \cdot ||\Sigma'_L \nabla_{x^{(L)} } \mathcal{L}||_2 \leq  \\ \leq\max_{l,i} \left( ||\Delta^{(l)}||_2  \cdot ||x_{(l-1)}||_2 \right) ||\Sigma'_L \nabla_{x^{(L)} } \mathcal{L}||_2
\end{equation*}

Weight initialization and normalization techniques such as layer or batch norm generally uniformize activations across layers. Assuming this <em>and</em> Lipschitz continuous activation
functions $\sigma^{(l)}$ we can bound first term above $ \max_{l,i} \left( ||\Delta^{(l)}||_2  \cdot ||x_{(l-1)}||_2 \right) \leq \rho$. Looking at gradient norm w.r.t. <em>all</em> network parameters we have

\begin{equation}
\label{eq:grad_norm_bound}
    ||\nabla_{\theta } \mathcal{L}||_2 \leq L \rho ||\Sigma'_L \nabla_{x^{(L)} } \mathcal{L}||_2 \equiv \hat{G}
\end{equation}

<em>
Eq. \ref{eq:grad_norm_bound} tells us variation of the gradient norm is mostly captured by the gradient of the loss function with respect to the pre-activation outputs of the last layer of our neural network. 
<br>
Computing this is $\approx$ as computationally expensive as a forward pass. 
</em>

<h2>When is Variance Reduction Possible?</h2>

We wouldn't want to always employ importance sampling using the upper bound in \ref{eq:grad_norm_bound}  as an estimate of the gradient norm:




<ul>
  <li>still significant computational overhead: requires forward passes for  <em>each</em> datapoint. </li>
  <li>would render this method unusable for online learning. </li>
  <li>would render this method unusable for online learning. </li>
</ul>

Two solutions address issues above: 



<ul>
  <li>first uniform sample a batch $B$,  <em>then</em> use importance sampling to obtain batch $b$. </li>
  <li>estimate what level of variance reduction is achievable without delaying computation to decide whether to use importance sampling. </li>
</ul>

<h3>Variance Reduction</h3>

Let $G_i$ be the gradient computed for data-point $i$. In SGD, for a batch of $B$ points, we sample from uniform distribution $u = \frac{1}{B}$ and compute an unbiased estimator with variance:

\begin{equation*}
    V_{init} \equiv \text{Tr} (\mathbb{V}_u [G_i] ) = \mathbb{E}_u \left[||G_i||_2^2 \right] -  (\mathbb{E}_u \left[G_i\right] )^2
\end{equation*}

With importance sampling we sample from distribution $g_i \propto ||G_i||_2$ and use weights $w_i = \frac{1}{B g_i}$ to compute unbiased estimator with variance:

\begin{equation*}
    V_{final} \equiv \text{Tr} (\mathbb{V}_g [G_i] ) = \mathbb{E}_g \left[w_i^2 \cdot ||G_i||_2^2 \right] -  (\mathbb{E}_g  \left[w_i \cdot G_i\right] )^2
\end{equation*}

Relative variance reduction is $\frac{V_i - V_f}{V_i}$ which after some algebra becomes:

\begin{equation}
\label{eq:variance_reduction}
    \frac{V_i - V_f}{V_i} = \frac{1}{\sum g_i^2} ||g-u||_2^2
\end{equation}

<em>
They quantify this variance reduction by thinking of the equivalent necessarily increase in batch size that would give same gains. Let this relative hypothetical batch size increase be $\tau$.
</em> <br> <br>

This improvement is equivalent to increasing our batch size $\tau$-fold where:

\begin{equation}
\label{eq:tau}
    1 - \frac{1}{\tau} = \frac{1}{\sum g_i^2} ||g-u||_2^2
\end{equation}

<h3>Computation</h3>

If the baseline is SGD with batch size $b$ we achieve performance gain in Eq. \ref{eq:variance_reduction} by either 
<ul>
  <li>use importance sampling to select batch $b$ from batch $B$. </li>
  <li>train with batch size $b\tau$ where $\tau$ is given by Eq. \ref{eq:tau}. </li>
</ul>
Paper assumes a backward pass is twice as expensive as a forward pass. The first option takes $\propto B + 3b$ operations, while the latter $\propto 3\tau b$. Importance sampling is desirable as long as $B+3b < 3\tau b$. In practice they observe speedups even for lower $\tau$'s.


<h2>Experiments</h2>

<ul>
  <li>Sampling with this upper bound estimate achieves variance reduction on-par to sampling using full gradient norm.  </li>
  <li>Sampling with loss heuristics sometimes completely fails.</li>
</ul>

<h3>Loss-based Sampling</h3>
Conjecture (supported by ablation studies) that sampling proportionally to the loss reduces the variance
only when the majority of the samples have losses close to
0.

<h1>References</h1>

[1] Katharopoulos, A. and Fleuret, F. Not all samples are created equal: Deep learning with importance sampling
<br>
[2] Alain, G., Lamb, A., Sankar, C., Courville, A., and Bengio, Y.  Variance Reduction in SGD by Distributed Importance Sampling
<br>
