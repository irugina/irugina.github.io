---
layout: post
---

<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
</style>


<h1>Variational Autoencoders</h1>


<h2>Introduction</h2>

<h4>Generative VS Discriminative Modeling</h4>

Generative models learn underlying world-model, while discriminative models learn direct map from input to target. In large-data regimes discriminative methods are probably better. Generative models work well in semi-supervised settings. They allow us to incorporate: 
<ul>
<li>relevant information</li>
<li>causal knowledge</li>
</ul>


<h4>Representation Learning</h4>

More generally, think of generative modeling as an auxiliary task that allows us to learn better representations that are:
<ul>
<li>disentangled</li>
<li>semantically meaningful</li>
<li>statistically independent</li>
<li>causal</li>
</ul>

<h4>Relation to Variational Inference (VI)</h4>

The inference module models latent variables as a stochastic function of input variables, making VAEs an amortized inference method. Sampling induces noise in gradients required for learning. Variance is reduced through reparameterization trick. VAEs thus marry graphical models and deep learning.

<h4>Comparison to GANs</h4>
VAEs are likelihood-based models and thus generally achieve worse sample quality but are better density estimators than GANs.

<h2>VAEs</h2>
Consist of two independetly parameterized models:
<ul>
<li>encoder or inference or recognition module $q_{\phi}(z|x)$ that approximates posterior over latent variables.</li>
<li>decoder or generative module $p_{\theta}(x,z) = p_{\theta}(x|z) \cdot p(z)$</li>
</ul>

<h4>ELBO</h4>
We want to maximize the marginal data likelihood or model evidence. For a single datapoint $x$:
\begin{equation*}
 \log p_{\theta} (x) = \mathbb{E}_{q_{\phi}(z|x)} \log p_{\theta} (x) = \mathbb{E}_{q_{\phi}(z|x)} \log \frac{p_{\theta} (x, z)}{p_{\theta}(z|x)} = \mathbb{E}_{q_{\phi}(z|x)} \log \frac{p_{\theta} (x, z)}{q_{\phi}(z|x)} \frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}
\end{equation*}

\begin{equation*}
 \log p_{\theta} (x) =  \mathbb{E}_{q_{\phi}(z|x)} \log \frac{p_{\theta} (x, z)}{q_{\phi}(z|x)} + \mathbb{E}_{q_{\phi}(z|x)} \log\frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}
\end{equation*}

First term is evidence lower bound or ELBO:
\begin{equation*}
\text{ELBO} = \mathbb{E}_{q_{\phi}(z|x)} \log \frac{p_{\theta} (x, z)}{q_{\phi}(z|x)}
\end{equation*}

Second term is KL divergence between approximate and real posterior:
\begin{equation*}
\text{D}_{\text{KL}}(q_{\phi}(z|x) | p_{\theta}(z|x)) = \mathbb{E}_{q_{\phi}(z|x)} \log\frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}
\end{equation*}

So this KL measures two distances:
<ul>
<li>distance between real and approximate posterior</li>
<li>distance between ELBO and marginal likelihood</li>
</ul>

We jointly optimize both $\phi$ and $\theta$ on the ELBO:
\begin{equation*}
\text{ELBO} = \mathbb{E}_{q_{\phi}(z|x)} \log \frac{p_{\theta} (x, z)}{q_{\phi}(z|x)} = \log p_{\theta} (x) - \text{D}_{\text{KL}}(q_{\phi}(z|x) | p_{\theta}(z|x))
\end{equation*}
Note this achieves two things at once:
<ul>
<li>maximize marginal likelihood</li>
<li>minimize discrepency between real and approximate posteriors</li>
</ul>

<h4>Stochastic Gradient-Based Optimization</h4>

Let $ \mathcal{L}_{\theta, \phi} (x) = \text{ELBO}(x)$. Optimization objective is sum of individual points' ELBO:
\begin{equation*}
\mathcal{L}_{\theta, \phi} (\mathcal{D}) = \sum_{x\in \mathcal{D}} \mathcal{L}_{\theta, \phi} (x)
\end{equation*}
<h6>Generative model parameters $\theta$</h6>
\begin{equation*}
\nabla_{\theta} \mathcal{L}_{\theta, \phi} (x) = \nabla_{\theta} \mathbb{E}_{q_{\phi}(z|x)} \log \frac{p_{\theta} (x, z)}{q_{\phi}(z|x)} = \nabla_{\theta} \mathbb{E}_{q_{\phi}(z|x)} \left [\log p_{\theta} (x, z) - \log q_{\phi}(z|x) \right] 
\end{equation*}
Since $\nabla_{\theta}$ and $\mathbb{E}_{q_{\phi}(z|x)}$ commute:
\begin{equation*}
\nabla_{\theta} \mathcal{L}_{\theta, \phi} (x) = \mathbb{E}_{q_{\phi}(z|x)}   \nabla_{\theta} \left [\log p_{\theta} (x, z) - q_{\phi}(z|x) \right] = \mathbb{E}_{q_{\phi}(z|x)}   \nabla_{\theta} \log p_{\theta} (x, z)
\end{equation*}
Unbiased MC estimator is given by $\nabla_{\theta} \mathcal{L}_{\theta, \phi} (x) \approx \nabla_{\theta} \log p_{\theta} (x, z) $ with $z \propto q_{\phi}(z|x)$.
<h6>Inference model parameters $\phi$</h6>
\begin{equation*}
\nabla_{\phi}\mathcal{L}_{\theta, \phi} (x) = \nabla_{\phi}\mathbb{E}_{q_{\phi}(z|x)} \left [\log p_{\theta} (x, z) - \log q_{\phi}(z|x) \right]
\end{equation*}
This is more tricky because $ \nabla_{\phi}$ and $\mathbb{E}_{q_{\phi}(z|x)} $ do not commute. Reparameterization Trick: 
\begin{equation*}
z \propto q_{\phi}(z|x) \rightarrow z = g(\epsilon, \phi, x)
\end{equation*}
Move the stochasticity to variable $\epsilon$ that is independent of $x$ or $\phi$. Now, $z$ is a deterministic variable and we have "externalized the randomness". ELBO becomes:
\begin{equation*}
\mathbb{E}_{q_{\phi}(z|x)} \left [\log p_{\theta} (x, z) - \log q_{\phi}(z|x) \right]  \rightarrow \mathbb{E}_{p (\epsilon)} \left [\log p_{\theta} (x, z) - \log q_{\phi}(z|x) \right] 
\end{equation*}
and inference parameters gradient:
\begin{equation*}
\nabla_{\phi}\mathbb{E}_{q_{\phi}(z|x)} \left [\log p_{\theta} (x, z) - \log q_{\phi}(z|x) \right] \rightarrow \mathbb{E}_{p (\epsilon)} \nabla_{\phi} \left [\log p_{\theta} (x, z) - \log q_{\phi}(z|x) \right] 
\end{equation*}
<h6>Putting it all together</h6>
Write ELBO as 
\begin{equation*}
\mathcal{L}_{\theta, \phi} (x) = \mathbb{E}_{p (\epsilon)} \left [\log p_{\theta} (x, z) - \log q_{\phi}(z|x) \right]
\end{equation*}
Unbiased Monte-Carlo estimator of single-point ELBO is:
\begin{equation*}
\widetilde{\mathcal{L}}_{\theta, \phi} (x) =\log p_{\theta} (x, z) - \log q_{\phi}(z|x)
\end{equation*}
with $\epsilon \propto p(\epsilon)$ and $z = g(\epsilon, \phi, x) $.
Applying SGD on this objective is the algorithm called  Auto-Encoding Variational Bayes (AEVB) and the reparameterized MC estimator is called  Stochastic Gradient Variational Bayes (SGVB) estimator. 

<h4>Computing $\log q_{\phi}(z|x)$</h4>
We need $\log q_{\phi}(z|x)$ above. With $z = g(\epsilon, \phi, x) $ and change of variable formula:
\begin{equation*}
z = g(\epsilon, \phi, x) \implies \log q_{\phi}(z|x) = \log p(\epsilon)- \log d_{\phi} (x, \epsilon)
\end{equation*}
where the jacobian $ \frac{\partial z}{\partial \epsilon}$ gives:
\begin{equation*} 
\log d_{\phi} (x, \epsilon) = \log \left |\det \frac{\partial z}{\partial \epsilon} \right |
\end{equation*}

<h6>Factorized Gaussian posteriors</h6>
\begin{equation*}
 q_{\phi}(z|x)= \mathcal{N}(z; \mu, \text{diag}(\sigma^2))
\end{equation*}
where $\mu$ and $\sigma$ are given by the encoder neural network as a function of $x$. After reparameterization:
\begin{equation*}
\epsilon \propto \mathcal{N}(0,1)
\end{equation*}
\begin{equation*}
 \mu, \log \sigma = \text{EncoderNeuralNet}_{\phi}(x)
\end{equation*}
\begin{equation*}
z = \mu + \sigma \times \epsilon
\end{equation*}
Now $\log q_{\phi}(z|x)$ becomes:
\begin{equation*}
\log q_{\phi}(z|x) = \log p(\epsilon)- \log d_{\phi} (x, \epsilon) =  \log p(\epsilon) - \log \left |\det \text{diag}(\sigma)\right |
\end{equation*}
\begin{equation*}
\log q_{\phi}(z|x) = \sum_i \left[ \log \mathcal{N}(\epsilon_i; 0,1) - \log \sigma_i \right]
\end{equation*}
<h6>Full-covariance Gaussian posterior</h6>
\begin{equation*}
 q_{\phi}(z|x)= \mathcal{N}(z; \mu, \Sigma)
\end{equation*}
Reparameterization that corresponds to Cholesky decomposition $\Sigma = L L^T$:
\begin{equation*}
\epsilon \propto \mathcal{N}(0,1)
\end{equation*}
\begin{equation*}
z = \mu + L\epsilon
\end{equation*}
which is nice because:
\begin{equation*}
\log d_{\phi} (x, \epsilon) = \sum_i \log |L_{ii}|
\end{equation*}

<h2>References</h2>
An Introduction to Variational Autoencoders: Diederik P. Kingma, Max Welling
