---
layout: post
---

<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
</style>

<h1> Matching Networks[1] </h1>

<h2> Method </h2>

Create map from a support set $S$ to a classifier $c_S(\cdot)$. Given a test example $\hat{x}$, $c_S(\hat{x})$ is a probability distribution over outputs $\hat{y}$. This map is parameterized by a neural network $P$ such that given a support set $S$ and a test point $\hat{x}$ we compute probability $P(\hat{y}|\hat{x}, S)$. Predicted class is $\arg\,max_y P(y|\hat{x}, S)$. Compute $\hat{y}$ as: 

\begin{equation*}
\hat{y} = \sum_{i=1}^k a(\hat{x}, x_i)y_i
\end{equation*}

where the sum is over support set examples. This method recovers both KDE and kNN.

<h3> Attention kernel</h3>

\begin{equation*}
a(\hat{x}, x_i) = \frac{e^{c \left (f(\hat{x}) g(x_i) \right)}} {\sum_{j=1}^k e^{c \left (f(\hat{x}) g(x_j) \right)}}
\end{equation*}

where $c$ is cosine distance, and $f$ and $g$ are embedding functions represented by NNs (potentially $f = g$) such as CNNs for images or word embedding for language tasks.


<h3> Contextualizing support set embeddings</h3>

This approach depends on whole support set $S$ through $P(\hat{y}|\hat{x}, S)$. However, we'd like the embeddings $f, g$ to also take into account the whole support set $S$ and not just embbed individual points. 

\begin{equation*}
g(x_i) \rightarrow g(x_i, S); \;\;\; f(\hat{x}) \rightarrow f(\hat{x}, S)
\end{equation*}

Solutions: 
<ul>
	<li>bidirectional LSTM to encode $x_i$ in the context of the support set S for $g(x_i, S)$ </li>
	<li>LSTM with read-attention over set S for $f(\hat{x}, S) = \text{attLSTM}(f'(\hat{x}), g(S), K)$ 
		<ul>
			<li>$f'(\hat{x})$ are features (e.g. from CNN)</li>
			<li>$g(S)$ is whole set embedded with $g$</li>
			<li>$K$ is the fixed number of unrolling steps of the LSTM</li>
		</ul>

	</li>
</ul>

<br><br><br><br>


<h1>Prototypical Networks[2]</h1>

<h2> Method</h2>

Create class prototypes $c_k$ using $N_s$ examples $x_i$ from support set:

<ul>
<li> few shot: $c_k = \frac{1}{N_s} \sum_{i=1}^{N_s}f_{\phi}(x_i)$  average embedding of class examples</li> 
<li> zero shot: use an embedding network for class meta-data $c_k = v_k$</li>
</ul>

Classify queries based on proximity to class prototypes using some distance metric $d$. For a query point $x$, produce a distribution: 

\begin{equation*}
p_{\phi}(y=k | x) = \frac{\exp \left( -d(f_{\phi}(x), c_k) \right)}{\sum_{k'} \exp \left( -d(f_{\phi}(x), c_{k'}) \right)}
\end{equation*}

Loss function is negative log-probability of true class:
\begin{equation*}
J(\phi) = - \log{p_{\phi} (y = k | x)}
\end{equation*}



<h2>Interpretations</h2>

Equivalent to matching networks[1] in the case of one-shot learning. 


<h3>Mixture Density Estimation</h3>

If metric $d$ is a Bregman divergence prototypical networks perform mixture density estimation with an exponential family distribution determined by $d$.

<h3>Linear Model</h3>

If metric $d$ is Euclidean distance the model is a linear classifier in embedding space.


<br><br><br><br>


<h1>Relation Networks[3]</h1>

<h2>Method </h2>
<ul>
	<li>Embedding module generates representations of the query and training images.</li>
	<li>Relation module compares embeddings: determine if they are from same categories. </li>
	<li>Embedding and relation modules are trained end-to-end. </li>
</ul>

This can be seen as extending the strategy of Prototypical networks or Matching networks to include a learnable non-linear comparator.


<br><br><br><br>
<h2>References</h2>


[1] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Matching networks for one shot learning. <br>

[2] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks for few-shot learning <br>

[3] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning <br>


